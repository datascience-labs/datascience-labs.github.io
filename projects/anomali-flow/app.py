import numpy as np
import streamlit as st
import requests
import pandas as pd
import time
import json
import tensorly as tl
from sklearn.preprocessing import StandardScaler
from tensorly.decomposition import parafac
from sklearn.preprocessing import MinMaxScaler,LabelEncoder
from sklearn.mixture import GaussianMixture
from bokeh.plotting import figure
from bokeh.palettes import Blues256
from bokeh.models import LinearColorMapper,Spacer,Range1d, ColorBar, ColumnDataSource, Legend, HoverTool, TableColumn, DataTable, CustomJS, TapTool
from bokeh.transform import transform, jitter
from bokeh.layouts import column, row
from prefect import flow, task
from celery.result import AsyncResult
from worker import run_timeseries_workflow, run_categorical_workflow, run_numerical_workflow

# ì‹œê°í™” íƒœìŠ¤í¬
#@task
#def create_visualizations(result, graph_type, x_column, y_column, df, selected_features):
def create_visualizations(result, graph_type, x_column, y_column, df, start_handle=None, end_handle=None):
    
    # outlier_indices ë³€ìˆ˜ê°€ ì‚¬ìš©ë˜ê¸° ì „ì— ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”
    outlier_indices = []
    
    outlier_indices = result.get('outlier_indices')
    outlier_probabilities = result.get('outlier_probabilities')
    root_cause_scores = result.get('root_cause_scores')
    index = result.get('index')
    
    # Root Cause Score íˆíŠ¸ë§µ ìƒì„±
    if root_cause_scores:
        # ì‹œê°í™”ë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
        timestamps = list(dict.fromkeys([str(ts) for ts in index if str(ts) in root_cause_scores]))
        features = list(next(iter(root_cause_scores.values())).keys())
        # íˆíŠ¸ë§µ ë°ì´í„° ìƒì„±
        heatmap_data = {
            'timestamp': [],
            'feature': [],
            'score': []
        }
    
        for timestamp in timestamps:
            for feature in features:
                score = root_cause_scores.get(str(timestamp), {}).get(feature, 0)
                heatmap_data['timestamp'].append(timestamp)
                heatmap_data['feature'].append(feature)
                heatmap_data['score'].append(score)
    
        # ì •ê·œí™”
        scaler = MinMaxScaler(feature_range=(0, 100))
        normalized_scores = scaler.fit_transform(np.array(heatmap_data['score']).reshape(-1, 1)).flatten()
        heatmap_data['score'] = normalized_scores
    
        source = ColumnDataSource(data=heatmap_data)
    
        # ìƒ‰ìƒ ë§¤í¼ ìƒì„±
        mapper = LinearColorMapper(palette="Blues256", low=100, high=0)
        
        # íˆíŠ¸ë§µ í”Œë¡¯ ìƒì„±
        p_heatmap = figure(
            title="Root Cause Scores Heatmap",
            x_range=timestamps,
            y_range=features,
            x_axis_label='Timestamp',
            y_axis_label='Feature',
            plot_width=1000,
            plot_height=400,
            tools="pan,wheel_zoom,box_zoom,reset",
            tooltips=[('Feature', '@feature'), ('Timestamp', '@timestamp'), ('Score', '@score')],
        )
    
        p_heatmap.rect(
            x="timestamp",
            y="feature",
            width=1,
            height=1,
            source=source,
            fill_color=transform('score', mapper),
            line_color=None
        )
    
        # ìƒ‰ìƒ ë°” ì¶”ê°€
        color_bar = ColorBar(color_mapper=mapper, location=(0, 0))
        p_heatmap.add_layout(color_bar, 'right')
    
        # íˆíŠ¸ë§µì„ Streamlitì— í‘œì‹œ
        st.bokeh_chart(p_heatmap)
        
        st.markdown("<br>", unsafe_allow_html=True)
        st.markdown("<hr>", unsafe_allow_html=True)

        
        st.markdown(
            "<p style='color:grey; font-size:12px; line-height:0.8;'>| outlierë¡œ íŒë³„ë˜ëŠ” ê·¼ë³¸ì ì¸ ì›ì¸ì„ ë¶„ì„í•˜ê³ , ê° ìš”ì†Œì˜ ê¸°ì—¬ë„ë¥¼ ì ìˆ˜ë¡œ ì‹œê°í™”í•œ ê·¸ë˜í”„ì…ë‹ˆë‹¤. </p>",
            unsafe_allow_html=True
        )
        
        # ì—¬ë°± ì¶”ê°€
        st.markdown("<br><br><br><br><br>", unsafe_allow_html=True)

    # ì¶”ê°€ ê·¸ë˜í”„
    outlier_indices_all = [i for i in outlier_indices if pd.notna(i) and i in df.index]
    inlier_indices_all = [i for i in df.index if i not in outlier_indices_all]
    
    inliers_all = df.loc[inlier_indices_all] if inlier_indices_all is not None else pd.DataFrame()
    outliers_all = df.loc[outlier_indices_all] if outlier_indices_all is not None else pd.DataFrame()
    
    # ì „ì²´ Dataì˜ ColumnDataSource ìƒì„±
    source_inliers_all = ColumnDataSource(data=dict(
        x=inlier_indices_all,
        y=inliers_all[y_column] if not inliers_all.empty else [],
    ))    
    
    source_outliers_all = ColumnDataSource(data=dict(
        x=outlier_indices_all if outlier_indices_all is not None else [],  # outlier ì¸ë±ìŠ¤ ì‚¬ìš©
        y=outliers_all[y_column] if not outliers_all.empty else [],  # outlier ê°’
    ))
    
    p_all = figure(title="Anomaly Score Graph", x_axis_label="Index", y_axis_label="Value", 
               plot_width=1000, plot_height=400,
               tools="pan,wheel_zoom,box_zoom,reset", 
               tooltips=[("Index", "@x"), ("Value", "@y")])
    
    # yì¶• ë²”ìœ„ ê³„ì‚° ë° í™•ì¥
    y_min, y_max = min(inliers_all[y_column].min(), outliers_all[y_column].min()), max(inliers_all[y_column].max(), outliers_all[y_column].max())
    y_extension = 2  # ì¶”ê°€í•˜ê³  ì‹¶ì€ ë²”ìœ„
    p_all.y_range = Range1d(y_min - y_extension, y_max + y_extension)
    
    p_all.title.align = "center"
    p_all.title.offset = 10
    p_all.title.text_font_style = "bold"
    p_all.title.text_font_size = "13pt"
    
    p_all.xaxis.axis_label_text_font_style = "italic"
    p_all.xaxis.axis_label_text_font_size = "10pt"
    
    p_all.yaxis.axis_label_text_font_style = "italic"
    p_all.yaxis.axis_label_text_font_size = "10pt"

    # ì „ì²´ ë°ì´í„°ì˜ ê·¸ë˜í”„ì— inliersì™€ outliers ê·¸ë¦¬ê¸°
    p_all.line(x='x', y='y', source=source_inliers_all, line_color='#3A7CA5', line_width=2, legend_label='Inliers')
    p_all.circle(x='x', y='y', source=source_outliers_all, color='#FF6B6B', size=6, legend_label='Outliers')
    
    # Anomaly Score Graph ì¶œë ¥
    st.bokeh_chart(p_all)
    
    st.markdown("<br>", unsafe_allow_html=True)
    st.markdown("<hr>", unsafe_allow_html=True)

    
    # ê·¸ë˜í”„ ìœ„ì— ì„¤ëª… ì¶”ê°€
    st.markdown(
        "<p style='color:grey; font-size:12px; line-height:0.8;'>| ì „ì²´ ë°ì´í„°ì—ì„œì˜ outlierì™€ inlierì˜ ë¶„í¬ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê·¸ë˜í”„ì…ë‹ˆë‹¤.</p>",
        unsafe_allow_html=True
    )
    
    # ì—¬ë°± ì¶”ê°€
    st.markdown("<br><br><br><br><br>", unsafe_allow_html=True)
    
    
    # ì…ë ¥ êµ¬ê°„(start_handle, end_handle)ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë§Œ ì„ íƒ
    if start_handle is None or end_handle is None:
        filtered_df = df  # ì „ì²´ ë°ì´í„° ì‚¬ìš©
    else:
        # ìœ„ì¹˜ ê¸°ë°˜ìœ¼ë¡œ ì…ë ¥ëœ ë²”ìœ„ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë¥¼ í•„í„°ë§
        filtered_df = df.iloc[df.index.get_loc(start_handle):df.index.get_loc(end_handle) + 1]


    # ì´ìƒì¹˜ì™€ ì •ìƒ ë°ì´í„° êµ¬ë¶„
    if outlier_indices is not None and len(outlier_indices) > 0:
        inliers = filtered_df.drop(outlier_indices)
        outliers = filtered_df.loc[outlier_indices]
    else:
        inliers = filtered_df
        outliers = pd.DataFrame()  # ë¹ˆ DataFrameìœ¼ë¡œ ì²˜ë¦¬


    # Inliersì™€ Outliersì— ëŒ€í•´ ê°ê°ì˜ ColumnDataSource ìƒì„±
    source_inliers = ColumnDataSource(data=dict(
        x=inliers[x_column], 
        y=inliers[y_column],
        **{feature: inliers[feature] for feature in inliers.columns if feature not in [x_column, y_column]}
    ))

    if not outliers.empty:
        outliers_data = {'x': outliers[x_column], 'y': outliers[y_column]}
        for col in outliers.columns:
            if col != x_column and col != y_column:
                outliers_data[col] = outliers[col]
        source_outliers = ColumnDataSource(data=outliers_data)
    else:
        source_outliers = None

    # Graph2 ì„¤ì •
    p_2d = figure(title=f"{x_column} vs {y_column}", 
                tools="pan,wheel_zoom,box_zoom,reset", 
                tooltips=[("X", "@x"), ("Y", "@y")])
    
    p_2d.title.align = "left"
    p_2d.title.offset = 10
    p_2d.title.text_font_style = "bold"
    p_2d.title.text_font_size = "13pt"
    
    p_2d.xaxis.axis_label_text_font_style = "italic"
    p_2d.xaxis.axis_label_text_font_size = "10pt"
    
    p_2d.yaxis.axis_label_text_font_style = "italic"
    p_2d.yaxis.axis_label_text_font_size = "10pt"

    # ê·¸ë˜í”„ íƒ€ì…ì— ë”°ë¥¸ ê·¸ë¦¬ê¸°
    if graph_type == "Line Graph":
        p_2d.line(x='x', y='y', source=source_inliers, line_color='#3A7CA5', line_width=2, legend_label='Inliers')
        if source_outliers is not None:
            p_2d.circle(x='x', y='y', source=source_outliers, color='#FF6B6B', size=6, legend_label='Outliers')

    elif graph_type == "Scatter Plot":
        p_2d.circle(x='x', y='y', source=source_inliers, color='#3A7CA5', size=6, legend_label='Inliers')
        if source_outliers is not None:
            p_2d.circle(x='x', y='y', source=source_outliers, color='#FF6B6B', size=6, legend_label='Outliers')

    elif graph_type == "Bar Graph":
        p_2d.vbar(x='x', top='y', source=source_inliers, width=0.9, color='#3A7CA5', alpha=0.8, legend_label='Inliers')
        if source_outliers is not None:
            p_2d.circle(x='x', y='y', source=source_outliers, color='#FF6B6B', size=6, legend_label='Outliers')
        p_2d.xgrid.grid_line_color = None
        p_2d.y_range.start = 0

    if graph_type == "Scatter Plot (Jittered)":
        # Inliers ë°ì´í„°ë¡œ Scatter Plot (Jittering) ìƒì„±
        p_2d.circle(x=jitter('x', width=0.1), y='y', source=source_inliers, color='#3A7CA5', size=6, legend_label='Inliers')
        # Outliers ë°ì´í„°ê°€ ì¡´ì¬í•˜ë©´ Outliersë„ í‘œì‹œ
        if source_outliers is not None:
            p_2d.circle(x=jitter('x', width=0.1), y='y', source=source_outliers, color='#FF6B6B', size=6, legend_label='Outliers')
            
        # xì¶• ë ˆì´ë¸”ì— ì›ë˜ ê°’ ì‚¬ìš© (LabelEncoder ì‚¬ìš©)
        if x_column in label_encoders:
            x_ticks = sorted(set(inliers[x_column]))
            x_labels = label_encoders[x_column].inverse_transform(x_ticks)
            
            x_labels = pd.Series(x_labels).fillna('Unknown').tolist()
            
            p_2d.xaxis.ticker = x_ticks
            p_2d.xaxis.major_label_overrides = {tick: label for tick, label in zip(x_ticks, x_labels)}

        # yì¶• ë ˆì´ë¸”ì— ì›ë˜ ê°’ ì‚¬ìš© (LabelEncoder ì‚¬ìš©)
        if y_column in label_encoders:
            y_ticks = sorted(set(inliers[y_column]))
            y_labels = label_encoders[y_column].inverse_transform(y_ticks)
            p_2d.yaxis.ticker = y_ticks
            p_2d.yaxis.major_label_overrides = {tick: label for tick, label in zip(y_ticks, y_labels)}

        # xì¶• ë° yì¶• ë ˆì´ë¸” ì„¤ì •
        p_2d.xaxis.axis_label = x_column
        p_2d.yaxis.axis_label = y_column

    # Hover tool ì¶”ê°€
    hover_tool_3 = HoverTool(tooltips=[('x', '@x'), ('y', '@y')], mode='vline')
    p_2d.add_tools(hover_tool_3)

    # í…Œì´ë¸”ì— ì¶œë ¥í•  ë°ì´í„°ë¥¼ ì €ì¥í•  ColumnDataSource ìƒì„± (ë¹ˆ ë°ì´í„°ë¡œ ì´ˆê¸°í™”)
    selected_point_features = ColumnDataSource(data=dict(Feature=[], Value=[]))

    # í…Œì´ë¸” ì»¬ëŸ¼ ì„¤ì •
    columns = [
        TableColumn(field="Feature", title="Feature"),
        TableColumn(field="Value", title="Value"),
    ]

    # DataTable ìƒì„±
    data_table = DataTable(source=selected_point_features, columns=columns, width=400, height=280)

    # TapTool ì¶”ê°€ ë° ì½œë°± ì—°ê²°
    tap_callback = CustomJS(args=dict(source_outliers=source_outliers, source_inliers=source_inliers, selected_source=selected_point_features), code="""
    console.log("Callback triggered");

    var outlier_selected_indices = source_outliers.selected.indices;
    var inlier_selected_indices = source_inliers.selected.indices;

    var data, selected_indices;

    if (outlier_selected_indices.length > 0) {
        selected_indices = outlier_selected_indices;
        data = source_outliers.data;
        source_inliers.selected.indices = [];
    } else if (inlier_selected_indices.length > 0) {
        selected_indices = inlier_selected_indices;
        data = source_inliers.data;
        source_outliers.selected.indices = [];
    }

    if (selected_indices.length > 0) {
        var index = selected_indices[0];
        var feature_names = Object.keys(data).filter(name => name !== 'x' && name !== 'y');
        var feature_values = feature_names.map(name => data[name][index]);

        var table_data = { Feature: [], Value: [] };
        for (var i = 0; i < feature_names.length; i++) {
            table_data['Feature'].push(feature_names[i]);
            table_data['Value'].push(feature_values[i]);
        }

        selected_source.data = table_data;
        selected_source.change.emit();
        console.log("Table updated");
    }
    """)

    if source_outliers is not None and source_inliers is not None:
        tap_tool = TapTool()
        p_2d.add_tools(tap_tool)
        source_outliers.selected.js_on_change('indices', tap_callback)
        source_inliers.selected.js_on_change('indices', tap_callback)
    
    # ê·¸ë˜í”„ì™€ ì„¤ëª… ì‚¬ì´ì— ì—¬ë°± ì¶”ê°€ (í•„ìš” ì‹œ)
    st.markdown("<br>", unsafe_allow_html=True) 
    

        
    layout = row(p_2d, data_table)
    st.bokeh_chart(layout)  
    st.markdown("<hr>", unsafe_allow_html=True)
    
    # ë‘ ë²ˆì§¸ ê·¸ë˜í”„ì™€ ë°ì´í„° í…Œì´ë¸”
    st.markdown(
        "<p style='color:grey;font-size:12px; line-height:0.8;'>| ì•ì„œ ì„ íƒí•œ ë‘ ê°œì˜ featureë¥¼ ê¸°ì¤€ìœ¼ë¡œ outlierì™€ inlierì˜ ë¶„í¬ë¥¼ ì‹œê°í™”í•œ ê·¸ë˜í”„ì…ë‹ˆë‹¤. </p>"
        "<p style='color:grey; font-size:12px; line-height:0.8;'>  ë” ìì„¸íˆ í™•ì¸í•˜ê³  ì‹¶ì€ pointë¥¼ í´ë¦­í•´ë³´ì„¸ìš”. ì˜¤ë¥¸ìª½ tableì—ì„œ ì„ íƒí•œ pointì— ëŒ€í•œ ì„¸ë¶€ì‚¬í•­ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>",
        unsafe_allow_html=True
    )
    
    st.markdown("<br><br><br><br><br>", unsafe_allow_html=True)

    


######## Time Series ë°ì´í„° Prefectì›Œí¬í”Œë¡œìš° and tasks ########
# ë°ì´í„° ë¡œë“œ ë° ìœ í˜• ê°ì§€
def classify_dataset(df):
    num_cols = len(df.columns)
    
    # Time series: ì‹œê°„ ê´€ë ¨ ì—´ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    time_series_cols = [col for col in df.columns if pd.api.types.is_datetime64_any_dtype(df[col]) or 
                        (df[col].dtype == 'object' and pd.to_datetime(df[col], errors='coerce').notna().any())]

    if time_series_cols:
        return 'time_series'

    # Categorical: ì—´ì˜ ê³ ìœ í•œ ê°’ì´ ì¼ì • ì„ê³„ì¹˜ ë¯¸ë§Œì´ë©´ categoricalë¡œ ë¶„ë¥˜
    categorical_count = sum(
        (df[col].dtype == 'object' or df[col].nunique() / len(df) < 0.05)
        for col in df.columns
    )
    if categorical_count / num_cols > 0.5:
        return 'categorical'

    # Numerical: ëŒ€ë¶€ë¶„ì˜ ì—´ì´ ìˆ«ìí˜•ì¸ ê²½ìš°
    numerical_count = sum(np.issubdtype(df[col].dtype, np.number) for col in df.columns)
    if numerical_count / num_cols > 0.5:
        return 'numerical' # 'numerical'
    
    return 'unknown'

# ë°ì´í„° ì „ì²˜ë¦¬
def timeseries_preprocess(df, tensor_rank, sliding_window_size):
    # 0ë²ˆì§¸ ì—´ì„ ë”°ë¡œ ì €ì¥ (ì›ë³¸ ë°ì´í„° ê·¸ëŒ€ë¡œ ì‚¬ìš©)
    first_col = df.iloc[:, 0]

    # ë‚˜ë¨¸ì§€ ì—´ë§Œ ì „ì²˜ë¦¬ ì§„í–‰
    df_processed = df.iloc[:, 1:].copy()
    
    for col in df_processed.columns:
        # ìˆ«ì ë³€í™˜ì´ í•„ìš”í•œ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ëª¨ë“  ì—´ì„ ìˆ«ìë¡œ ë³€í™˜
        df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')  # ë³€í™˜í•  ìˆ˜ ì—†ëŠ” ê°’ì„ NaNìœ¼ë¡œ ì²˜ë¦¬
        df_processed[col] = df_processed[col].fillna(df_processed[col].median())

        # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì ìš©
        data = df_processed[col].to_numpy()
        T = len(data)
        N = T - sliding_window_size + 1

        if N <= 0:
            continue

        sliding_windows = np.array([data[i:i + sliding_window_size] for i in range(N)])
        sliding_windows = np.mean(sliding_windows, axis=1)
        df_processed[col] = pd.Series(sliding_windows, index=df_processed.index[:len(sliding_windows)])

    # NaN ê°’ì„ ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´
    df_processed = df_processed.fillna(df_processed.median())

    # 0ë²ˆì§¸ ì—´ì„ ë‹¤ì‹œ ê²°í•©í•˜ì—¬ ë°˜í™˜
    df_processed.insert(0, first_col.name, first_col)

    return df_processed



def categorical_preprocess(df):
    # 0ë²ˆì§¸ ì—´ì„ ë”°ë¡œ ì €ì¥ (ì›ë³¸ ë°ì´í„° ê·¸ëŒ€ë¡œ ì‚¬ìš©)
    first_col = df.iloc[:, 0]

    # ë‚˜ë¨¸ì§€ ì—´ë§Œ ì „ì²˜ë¦¬ ì§„í–‰
    df_processed = df.iloc[:, 1:].copy()
    
    label_encoders = {}
    for column in df_processed.columns:
        # Categorical ì—´ì¸ì§€ í™•ì¸
        if df_processed[column].dtype == 'object' or pd.api.types.is_categorical_dtype(df_processed[column]):
            # LabelEncoderë¥¼ ì‚¬ìš©í•´ ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ìˆ«ìë¡œ ë³€í™˜
            le = LabelEncoder()
            df_processed[column] = le.fit_transform(df_processed[column])
            label_encoders[column] = le  # ë‚˜ì¤‘ì— ë³€í™˜í•  ë•Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì €ì¥

    # 0ë²ˆì§¸ ì—´ì„ ë‹¤ì‹œ ê²°í•©í•˜ì—¬ ë°˜í™˜
    df_processed.insert(0, first_col.name, first_col)

    return df_processed, label_encoders

def numerical_preprocess(df, tensor_rank):
    # 0ë²ˆì§¸ ì—´ì„ ë”°ë¡œ ì €ì¥ (ì›ë³¸ ë°ì´í„° ê·¸ëŒ€ë¡œ ì‚¬ìš©)
    first_col = df.iloc[:, 0]

    # ë‚˜ë¨¸ì§€ ì—´ë§Œ ì „ì²˜ë¦¬ ì§„í–‰
    df_processed = df.iloc[:, 1:].copy()
    scaler = StandardScaler()

    for col in df_processed.columns:
        # ì‘ì€ ë”°ì˜´í‘œë¡œ ë¬¶ì¸ ë¬¸ìì—´ì„ ì²˜ë¦¬í•˜ì—¬ floatìœ¼ë¡œ ë³€í™˜
        if df_processed[col].dtype == object:  # ë¬¸ìì—´ë¡œ ì¸ì‹ë˜ëŠ” ê²½ìš°
            df_processed[col] = df_processed[col].str.replace("'", "").astype(float)

        # ì—´ì´ numericalì¸ì§€ í™•ì¸ (ìˆ«ìí˜• ì—´ë§Œ í…ì„œ ë¶„í•´ ì ìš©)
        if np.issubdtype(df_processed[col].dtype, np.number):
            data = df_processed[col].to_numpy().reshape(-1, 1)

            # ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ì ìš©
            scaled_data = scaler.fit_transform(data)

            # í…ì„œ ë¶„í•´ ì ìš©
            tensor = tl.tensor(scaled_data)
            factors = parafac(tensor, rank=tensor_rank)
            reconstructed_tensor = tl.kruskal_to_tensor(factors)
            df_processed[col] = pd.Series(reconstructed_tensor.flatten(), index=df_processed.index)

        # NaN ê°’ ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´ (ìˆ«ìí˜• ì—´ë§Œ)
        df_processed[col] = df_processed[col].fillna(df_processed[col].median())

    # 0ë²ˆì§¸ ì—´ì„ ë‹¤ì‹œ ê²°í•©í•˜ì—¬ ë°˜í™˜
    df_processed.insert(0, first_col.name, first_col)

    return df_processed


# Prefect íƒœìŠ¤í¬: Celeryì— ì‘ì—…ì„ ë„˜ê¸°ëŠ” í•¨ìˆ˜
@task(log_prints=True)
def submit_to_celery(df, algorithm, params, workflow_type):
    if workflow_type == 'timeseries':
        task = run_timeseries_workflow.apply_async(args=[df, algorithm, params])
    elif workflow_type == 'categorical':
        task = run_categorical_workflow.apply_async(args=[df, algorithm, params])
    elif workflow_type == 'numerical':
        task = run_numerical_workflow.apply_async(args=[df, algorithm, params])
    
    return task.id  # ì‘ì—… ID ë°˜í™˜

# Prefect íƒœìŠ¤í¬: Celery ì‘ì—…ì˜ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
@task(log_prints=True)
def get_celery_result(task_id):
    task_result = AsyncResult(task_id)
    if task_result.ready():
        return task_result.result
    else:
        return "Task is still running..."

######## Time Series  ë°ì´í„° Prefect ì›Œí¬í”Œë¡œìš° ########
@flow(log_prints=True)
def timeseries_workflow(df, algorithm, param):
    # Celery workerì— ì‘ì—…ì„ ë„˜ê¹€
    task_id = submit_to_celery(df, algorithm, params, 'timeseries')
    # Celery ì‘ì—…ì˜ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜´
    result = get_celery_result(task_id)
    return result

######## Categorical  ë°ì´í„° Prefect ì›Œí¬í”Œë¡œìš° ########
@flow(log_prints=True)
def categorical_workflow(df, algorithm, params):
    # Celery workerì— ì‘ì—…ì„ ë„˜ê¹€
    task_id = submit_to_celery(df, algorithm, params, 'categorical')
    # Celery ì‘ì—…ì˜ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜´
    result = get_celery_result(task_id)
    return result

######## Numerical  ë°ì´í„° Prefect ì›Œí¬í”Œë¡œìš° ########
@flow(log_prints=True)
def numerical_workflow(df, algorithm, params):
    # Celery workerì— ì‘ì—…ì„ ë„˜ê¹€
    task_id = submit_to_celery(df, algorithm, params, 'numerical')
    # Celery ì‘ì—…ì˜ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜´
    result = get_celery_result(task_id)
    return result

######## ì‹œê°í™” ########
@flow
def visualization_flow(result, graph_type, x_column, y_column, df, start_handle=None, end_handle=None):
    create_visualizations(result, graph_type, x_column, y_column, df, start_handle, end_handle)

st.set_page_config(layout="wide")  # í™”ë©´ì„ ë„“ê²Œ ì‚¬ìš©

# ì„¸ì…˜ ìƒíƒœ ìœ ì§€ (CSV íŒŒì¼ ë° ëª¨ë¸ ì‹¤í–‰ ìƒíƒœ ê´€ë¦¬)
if 'uploaded_file' not in st.session_state:
    st.session_state.uploaded_file = None

# ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° í™•ì¸
if 'model_run' not in st.session_state:
    st.session_state.model_run = False

# ê¸°ë³¸ì ìœ¼ë¡œëŠ” Configuration íƒ­ë§Œ ì¡´ì¬
if st.session_state.model_run:
    tabs = st.tabs(["Configuration Page", "Visualization Page"])
else:
    tabs = st.tabs(["Configuration Page"])

if __name__ == "__main__":
    with tabs[0]:
        # Streamlit ì¸í„°í˜ì´ìŠ¤
        col1, col2 = st.columns(2)
        with col1:
            # st.set_page_config(page_title="AnomaliFlow: Distributed Execution of Reusable ML Workflows", page_icon=":material/edit:")
            st.title("AnomaliFlow")
            
            st.markdown("""
                        AnomaliFlowëŠ” ë¶„ì‚° í™˜ê²½ì—ì„œ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ë¨¸ì‹ ëŸ¬ë‹ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ê°•ë ¥í•œ ë„êµ¬ì…ë‹ˆë‹¤. ì´ í”Œë«í¼ì€ ë³µì¡í•œ ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸ì„ ì†ì‰½ê²Œ êµ¬ì„±í•˜ê³ , ì´ë¥¼ ì—¬ëŸ¬ ì»´í“¨íŒ… ë…¸ë“œì— ë¶„ì‚°í•˜ì—¬ ë¹ ë¥´ê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ë•ìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ê³¼ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ íš¨ê³¼ì ìœ¼ë¡œ ê²°í•©í•˜ê³ , ìœ ì—°í•œ ì‹¤í–‰ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ë†’ì€ ìƒì‚°ì„±ì„ ì œê³µí•©ë‹ˆë‹¤.

                        ì£¼ìš” ê¸°ëŠ¥:
                        - **ë¶„ì‚° ì»´í“¨íŒ… ì§€ì›**: ì—¬ëŸ¬ ë…¸ë“œì—ì„œ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í†µí•´ ëŒ€ìš©ëŸ‰ ë°ì´í„° ë° ë³µì¡í•œ ëª¨ë¸ë„ ë¹ ë¥´ê²Œ ì²˜ë¦¬ ê°€ëŠ¥í•©ë‹ˆë‹¤.
                        - **ì›Œí¬í”Œë¡œìš° ì¬ì‚¬ìš©ì„±**: ë°˜ë³µì ì¸ ì‘ì—…ì„ ìë™í™”í•˜ê³ , ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œë„ ë™ì¼í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‰½ê²Œ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                        - **í™•ì¥ì„±**: ë‹¤ì–‘í•œ ML í”„ë ˆì„ì›Œí¬ ë° íˆ´ê³¼ì˜ í†µí•©ì„ ì§€ì›í•˜ì—¬ í™•ì¥ì„±ê³¼ ìœ ì—°ì„±ì„ ì œê³µí•©ë‹ˆë‹¤.

                        ì´ í”Œë«í¼ì„ í†µí•´ ë³´ë‹¤ íš¨ìœ¨ì ì´ê³  ê°„í¸í•œ ë¨¸ì‹ ëŸ¬ë‹ ê°œë°œì„ ê²½í—˜í•´ë³´ì„¸ìš”.
                        """)
            # sidebar
            with st.sidebar:
                st.title("AnomaliFlow")
                st.markdown("""
                            Distributed Execution of Reusable ML Workflows
                            """)
                st.divider()
                st.header("ğŸ’» ì£¼ìš” ê¸°ëŠ¥")
                stage = st.sidebar.button('About')
                                        
                st.header("âš™ ML Models") 
                stage = st.sidebar.button('Supported ML Models')

                # http://localhost:4200/dashboard ì™€ì˜ ì—°ë™
                st.header("ğŸ“Š Workflow Management")
                stage = st.sidebar.radio("Choose Step", ['Home', 'Saved Workflows', 'Monitor Workflows'])

                st.header("ë§Œë“  ì‚¬ëŒ")
                stage = st.sidebar.button('Our team')
                

            st.header("Data import" )
            uploaded_file = st.file_uploader("Choose a CSV file", type="csv")

            st.header("Jobs for Executing Distributed Task", divider=True)
            n_jobs = st.slider("Select n_jobs", min_value=1, max_value=6, value=2)

            st.write(f"n_jobs: {n_jobs}")

            if uploaded_file is not None:
                df = pd.read_csv(uploaded_file, encoding="UTF-8")
                st.write(df)

                # Detect data types
                data_type = classify_dataset(df)
                st.write(data_type)
                with col2:        
                    st.header(f"Type of the Dataset: {data_type.capitalize()}", divider=True)
                    
                    model_selections = {}

                    # Time Series Data
                    if data_type == 'time_series':
                        
                        st.subheader(f"Model Configuration", divider=True)
                        # Hyper-parameters for Tensor Decomposition and Window Operation
                        tensor_rank = st.slider("Tensor Rank", min_value=1, max_value=10, value=3)
                        sliding_window_size = st.slider("Sliding Window Size", min_value=2, max_value=50, value=5)

                        # Data Preprocessing
                        df = timeseries_preprocess(df, tensor_rank, sliding_window_size)

                        # Model Selection
                        time_series_model = st.selectbox("Select a Model", ["IsolationForest", "GMM"])
                        
                        
                        # # ì–‘ë°©í–¥ ìŠ¬ë¼ì´ë”
                        # preview_handle_range = st.slider(
                        #     "Filtering range for data points", 
                        #     1, len(df), 
                        #     value=(1, 1500)  # ì´ˆê¸°ê°’ ë²”ìœ„ ì„¤ì • (ì‹œì‘ì , ëì )
                        # )

                        # # íƒ€ì„ í•„í„°ë§
                        # start_handle, end_handle = map(int, preview_handle_range)
                        # selected_data = df[start_handle:end_handle]

                        # # threshold_handle = st.slider("Threshold", 0.0, 1.0, value=0.5)
                        
                        params = {}
                        if time_series_model == "IsolationForest":
                            params['max_samples'] = st.number_input("The maximum number of samples", 1, len(df))
                            params['n_estimators'] = st.number_input("The number of estimators", 1, 1000, value=100)
                            params['contamination'] = st.number_input("The contamination parameter", 0.0, 0.5, value=0.1)
                            params['n_jobs'] = n_jobs
                        elif time_series_model == "GMM":
                            params['n_init'] = st.slider("The number of times for the GMM execution with different certroid seeds", min_value=1, max_value=10, value=1)
                            params['n_components'] = st.number_input("The number of components", 1, len(df), value=2)
                            # params['covariance_type'] = st.selectbox("Covariance type", ["full", "tied", "diag", "spherical"], index=0)
                            params['random_state'] = st.number_input("Random state", 0, 1000, value=42)
                            params['init_params'] = 'kmeans'
                        
                        st.subheader(f"Visualization", divider=True)
                        graph_type = st.selectbox("Select a Graph Type", ["Line Graph", "Scatter Plot", "Bar Graph"])
                        columns = df.columns.tolist()
                        columns = [col for col in df.columns if col != 'date']
                        x_column = st.selectbox("Select X-axis Feature", columns)
                        y_column = st.selectbox("Select Y-axis Feature", columns)
                        
                        if st.button("Run your workflow"):
                            st.session_state.model_run = True
                            if len(tabs) > 1 and st.session_state.model_run:
                                with tabs[1]:
                                    st.title("AnomaliFlow Visualization")
                                    # Celery workerì— ë¹„ë™ê¸° ì‘ì—… ìš”ì²­
                                    df_dict = df.to_dict(orient='records')
                                    result = run_timeseries_workflow(df_dict, time_series_model, params)
                                    
                                    # Visualization step
                                    if result:
                                        st.write("Workflow Completed! Visualizing Results...")
                                        # Call visualization function
                                        visualization_flow(result, graph_type, x_column, y_column, df, None, None)
                                        st.write(result)
                                        #visualization_flow(result, graph_type, x_column, y_column, df, start_handle, end_handle)    
                    
                    # Categorical Data
                    elif data_type == 'categorical':
                        
                        st.subheader(f"Model Configuration", divider=True)
                        # Data Preprocessing
                        df, label_encoders = categorical_preprocess(df)

                        # Algorithm select (ë°ì´í„° ìœ í˜•ë³„ë¡œ ë‹¤ë¥´ê²Œ)
                        algorithm = st.selectbox("Select a Model", ["DBSCAN", "LOF"])

                        # threshold_handle = st.slider("Threshold", 0.0, 1.0, value=0.5)

                        # parameters
                        params = {}
                        if algorithm == "DBSCAN":
                            eps = st.slider("epsilon(Æ)", min_value=0.01, max_value=10.00, value=0.05)
                            min_samples = st.slider("The mininum number of samples", min_value=1, max_value=100, value=5)
                            params = {"eps": eps, "min_samples": min_samples, "n_jobs" : n_jobs}
                        elif algorithm == "LOF":
                            params['n_neighbors'] = st.number_input("The number of neighbors", 1, 100, value=20)
                            params['contamination'] = st.number_input("The contamination parameter", 0.0, 0.5, value=0.1)
                            params['n_jobs'] = n_jobs


                        st.subheader(f"Visualization", divider=True)
                        graph_type = st.selectbox("Select a Type of Chart ", ["Line Graph", "Scatter Plot", "Bar Graph", "Scatter Plot (Jittered)"])
                        columns = df.columns.tolist()
                        x_column = st.selectbox("Select X-axis Feature", columns)
                        y_column = st.selectbox("Select Y-axis Feature", columns)

                        if st.button("Run your workflow"):
                            st.session_state.model_run = True
                            if st.session_state.model_run:
                                with tabs[1]:
                                    st.title("AnomaliFlow Visualization")
                                    # Celery workerì— ë¹„ë™ê¸° ì‘ì—… ìš”ì²­
                                    df_dict = df.to_dict(orient='records')
                                    result = run_categorical_workflow(df_dict, algorithm, params)
                                    
                                    # Visualization step
                                    if result:
                                        st.write("Workflow Completed! Visualizing Results...")
                                        st.write(result)
                                        # Call visualization function
                                        visualization_flow(result, graph_type, x_column, y_column, df, None, None)

                    # Numerical Data
                    if data_type == 'numerical':
                        
                        st.subheader(f"Model Configuration", divider=True)
                        # Hyper-parameters for Tensor Decomposition 
                        tensor_rank = st.slider("Tensor Rank", min_value=1, max_value=10, value=1)

                        # Data Preprocessing
                        df = numerical_preprocess(df, tensor_rank)

                        # Model Selection
                        numerical_model = st.selectbox("Select a Model", ["IsolationForest", "GMM", "DBSCAN", "LOF", "KMeans"])

                        # threshold_handle = st.slider("Threshold", 0.0, 1.0, value=0.5)
                        
                        params = {}
                        if numerical_model == "IsolationForest":
                            params['max_samples'] = st.number_input("The maximum number of samples", 1, len(df))
                            params['n_estimators'] = st.number_input("The number of estimators", 1, 1000, value=100)
                            params['contamination'] = st.number_input("The contamination parameter", 0.0, 0.5, value=0.1)
                            params['n_jobs'] = n_jobs
                        elif numerical_model == "GMM":
                            params['n_init'] = st.slider("The number of times for the GMM execution with different certroid seeds", min_value=1, max_value=10, value=1)
                            params['n_components'] = st.number_input("The number of components", 1, len(df), value=2)
                            # params['covariance_type'] = st.selectbox("Covariance type", ["full", "tied", "diag", "spherical"], index=0)
                            params['random_state'] = st.number_input("Random state", 0, 1000, value=42)
                            params['init_params'] = 'kmeans'
                        elif numerical_model == "DBSCAN":
                            eps = st.slider("epsilon(Æ)", min_value=0.01, max_value=10.00, value=0.05)
                            min_samples = st.slider("The mininum number of samples", min_value=1, max_value=100, value=5)
                            params = {"eps": eps, "min_samples": min_samples, "n_jobs" : n_jobs}
                        elif numerical_model == "LOF":
                            params['n_neighbors'] = st.number_input("The number of neighbors", 1, 100, value=20)
                            params['contamination'] = st.number_input("The contamination parameter", 0.0, 0.5, value=0.1)
                            params['n_jobs'] = n_jobs
                        elif numerical_model == "KMeans":
                            n_clusters = st.slider("The number of clusters", min_value=2, max_value=20, value=3)
                            n_init = st.slider("The number of times for the KMeans execution with different certroid seeds", min_value=1, max_value=20, value=10)
                            params = {"n_clusters": n_clusters, "n_init": n_init, "n_jobs" : n_jobs}

                    
                        
                        st.subheader(f"Visualization", divider=True)
                        graph_type = st.selectbox("Select a Graph Type", ["Line Graph", "Scatter Plot", "Bar Graph"])
                        columns = df.columns.tolist()
                        x_column = st.selectbox("Select X-axis Feature", columns)
                        y_column = st.selectbox("Select Y-axis Feature", columns)
                        
                        if st.button("Run your workflow"):
                            st.session_state.model_run = True
                            if st.session_state.model_run:
                                with tabs[1]:
                                    st.title("AnomaliFlow Visualization")
                                    # Celery workerì— ë¹„ë™ê¸° ì‘ì—… ìš”ì²­
                                    df_dict = df.to_dict(orient='records')
                                    result = run_numerical_workflow(df_dict, numerical_model, params)
                                    
                                    # Visualization step
                                    if result:
                                        st.write("Workflow Completed! Visualizing Results...")
                                        st.write(result)
                                        # Call visualization function
                                        visualization_flow(result, graph_type, x_column, y_column, df, None, None)
